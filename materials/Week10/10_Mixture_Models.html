<!DOCTYPE html>
<html lang="en"><head>
<script src="10_Mixture_Models_files/libs/clipboard/clipboard.min.js"></script>
<script src="10_Mixture_Models_files/libs/quarto-html/tabby.min.js"></script>
<script src="10_Mixture_Models_files/libs/quarto-html/popper.min.js"></script>
<script src="10_Mixture_Models_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="10_Mixture_Models_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="10_Mixture_Models_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="10_Mixture_Models_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.269">

  <meta name="author" content="Lecture 10">
  <title>Mixture Models and Latent Class Analysis</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="10_Mixture_Models_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="10_Mixture_Models_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="10_Mixture_Models_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="10_Mixture_Models_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="10_Mixture_Models_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="10_Mixture_Models_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="10_Mixture_Models_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Mixture Models and Latent Class Analysis</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Multidimensional Measurement Models (Fall 2023): Lecture 10 
</div>
</div>
</div>

</section>
<section id="lecture-outline" class="slide level2">
<h2>Lecture Outline</h2>
<ul>
<li>Latent Class Analysis (LCA) Underlying theory (general contexts)</li>
<li>Example analysis (and how to get estimates)</li>
<li>Interpretation of model parameters</li>
<li>Investigating model fit Extensions of the Technique: Latent Profile Analysis (LPA)</li>
</ul>
</section>
<section id="clusters-versus-classes" class="slide level2">
<h2>Clusters Versus Classes</h2>
<ul>
<li>When a researcher mentions they are going to be using cluster analysis, they are most likely referring to one of the following:
<ul>
<li>K-means clustering Hierarchical clustering using distance methods</li>
<li>Discriminant analysis</li>
<li>Taxometrics</li>
</ul></li>
<li>Much less often, latent class analysis is included in the group
<ul>
<li>Although it too is useful for detecting clusters of observations</li>
</ul></li>
<li>For today’s lecture, we will consider clusters to be synonymous with classes</li>
</ul>
</section>
<section id="lca-versus-other-methods" class="slide level2">
<h2>LCA Versus Other Methods</h2>
<ul>
<li><p>Although I am using the terms classes and clusters synonymously, the general approach of LCA differs from that of the other methods previously discussed</p></li>
<li><p>LCA is a model-based method for clustering (or classification)</p>
<ul>
<li>LCA fits a statistical model to the data in an attempt to determine classes</li>
</ul></li>
<li><p>The other methods listed on the previous slide do not explicitly state a statistical model</p></li>
<li><p>By being model based, we are making very explicit assumptions about our data</p>
<ul>
<li>Assumptions that can be tested</li>
</ul></li>
</ul>
</section>
<section>
<section id="latent-class-analysis" class="title-slide slide level1 center">
<h1>Latent Class Analysis</h1>

</section>
<section id="lca-introduction" class="slide level2">
<h2>LCA Introduction</h2>
<ul>
<li>Latent class models are commonly attributed to Lazarsfeld and Henry (1968)</li>
<li>The final number of classes is not usually predetermined prior to analysis with LCA
<ul>
<li>The number of classes is determined through comparison of posterior fit statistics</li>
<li>The characteristics of each class is also determined following the analysis</li>
<li>Similar to K-means and hierarchical clustering techniques in this respect</li>
</ul></li>
</ul>
</section>
<section id="variable-types-used-in-lca" class="slide level2">
<h2>Variable Types Used in LCA</h2>
<ul>
<li>As it was originally conceived, LCA is an analysis that uses:
<ul>
<li>A set of binary-outcome variables - values coded as zero or one</li>
</ul></li>
<li>Examples include:
<ul>
<li>Test items - scored correct or incorrect</li>
<li>True/false questions</li>
<li>Gender</li>
<li>Anything else that has two possible outcomes</li>
</ul></li>
</ul>
</section>
<section id="lca-process" class="slide level2">
<h2>LCA Process</h2>
<ul>
<li>For a specified number of classes, LCA attempts to:
<ul>
<li>For each class, estimate the probability that each variable is equal to one</li>
<li>Estimate the probability that each observation falls into each class
<ul>
<li>For each observation, the sum of these probabilities across classes equals one</li>
<li>This is different from K-means where an observation is a member of a class with certainty</li>
</ul></li>
<li>Across all observations, estimate the probability that <em>any</em> observation falls into a class</li>
</ul></li>
</ul>
</section>
<section id="lca-estimation" class="slide level2">
<h2>LCA Estimation</h2>
<ul>
<li>Estimation of LCA model parameters can be more complicated than other clustering methods:
<ul>
<li>In hierarchical clustering, a search process is used with new distance matrices being created for each step</li>
<li>K-means uses more of a brute-force approach - trying multiple random starting points then shifting cases between the different clusters until each is no longer shifted</li>
<li>Both methods relied on distance metrics to find clustering solutions</li>
</ul></li>
<li>LCA estimation uses distributional assumptions to find classes</li>
<li>The distributional assumptions provide the measure of “distance” in LCA</li>
</ul>
</section>
<section id="lca-distributional-assumptions" class="slide level2">
<h2>LCA Distributional Assumptions</h2>
<ul>
<li>Because (for today) we have discussed LCA with binary-outcome variables, the distributional assumptions of LCA must use a binary-outcome distribution</li>
<li>Within each latent class, the variables are assumed to:
<ul>
<li>Be independent</li>
<li>Be distributed marginally as Bernoulli:
<ul>
<li>The Bernoulli distribution PMF is:</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[f(Y_{pi}) = \left(\pi_i \right)^{Y_{pi}} \left(1-\pi_i\right)^{(1-Y_{pi})}\]</span></p>
<ul>
<li>The Bernoulli distribution is a simple distribution for a single event - like flipping a coin</li>
</ul>
</section>
<section id="bernoulli-distribution-illustration" class="slide level2">
<h2>Bernoulli Distribution Illustration</h2>
<ul>
<li>To illustrate the Bernoulli distribution (and statistical likelihoods in general), consider the following example</li>
<li>To illustrate the Bernoulli distribution, consider a prediction of the result of the Northwester / Iowa football game as a binary-response item, <span class="math inline">\(Y\)</span>
<ul>
<li>Let’s say <span class="math inline">\(Y=1\)</span> if Iowa wins, and <span class="math inline">\(Y=0\)</span> if Northwestern wins</li>
<li>My prediction is that Iowa has about an 87% chance of winning the game</li>
<li>So, <span class="math inline">\(\pi=0.87\)</span></li>
</ul></li>
<li>Likewise, <span class="math inline">\(P(Y=1)=0.87\)</span> and <span class="math inline">\(P(Y=0)=0.13\)</span>.</li>
</ul>
</section>
<section id="bernoulli-distribution-illustration-1" class="slide level2">
<h2>Bernoulli Distribution Illustration</h2>
<ul>
<li><p>The likelihood function for <span class="math inline">\(Y\)</span> looks similar:</p></li>
<li><p>If <span class="math inline">\(Y=1\)</span>, the likelihood is: <span class="math display">\[f(Y=1) = \left(0.87 \right)^{1} \left(1-0.87\right)^{(1-1)} =0.87\]</span></p></li>
<li><p>If <span class="math inline">\(Y=0\)</span>, the likelihood is: <span class="math display">\[f(Y=0) = \left(0.87 \right)^{1} \left(1-0.87\right)^{(1-0)} =0.13\]</span></p></li>
<li><p>This example shows you how the likelihood function of a statistical distribution gives you the likelihood of an event occurring</p></li>
<li><p>In the case of discrete-outcome variables, the likelihood of an event is synonymous with the probability of the event occurring</p></li>
</ul>
</section>
<section id="independent-bernoulli-variables" class="slide level2">
<h2>Independent Bernoulli Variables</h2>
<ul>
<li>To consider what independence of Bernoulli variables means, let’s consider the another game this season: Michigan v. Ohio State</li>
<li>Let’s say we predict Michigan has a 57% chance of winning (or <span class="math inline">\(\pi_2=0.57\)</span>).</li>
<li>By assumption of independence of games, the probability of both Iowa and Michigan wining their games would be the product of the probability of winning each game separately:</li>
</ul>
<p><span class="math display">\[P(Y_1=1,Y_2=1) = \pi_1 \pi_2 = 0.87 \times 0.57 = 0.4959\]</span></p>
<ul>
<li>More generally, we can express the likelihood of any set of occurrences by:</li>
</ul>
<p><span class="math display">\[P(Y_1=y_1, Y_2=y_2,\ldots,Y_I=y_I) = \prod_{i=1}^{I} \pi_i^{Y_i}\left(1-\pi_i\right)^{\left(1-Y_i\right)}\]</span></p>
</section></section>
<section>
<section id="finite-mixture-models" class="title-slide slide level1 center">
<h1>Finite Mixture Models</h1>

</section>
<section id="finite-mixture-models-1" class="slide level2">
<h2>Finite Mixture Models</h2>
<ul>
<li>LCA models are special cases of more general models called Finite Mixture Models</li>
<li>A finite mixture model expresses the distribution of a set of outcome variables, <span class="math inline">\(\boldsymbol{Y}\)</span>, as a function of the sum of weighted distribution likelihoods:</li>
</ul>
<p><span class="math display">\[f(\textbf{Y}) = \sum_{c=1}^C \eta_c f(\textbf{Y}|c)\]</span></p>
<ul>
<li>With this general form we can now construct the (more specific) LCA model likelihood</li>
<li>Here, we say that the conditional distribution of <span class="math inline">\(\boldsymbol{Y}\)</span> given <span class="math inline">\(c\)</span> is a set of independent Bernoulli variables</li>
</ul>
</section>
<section id="latent-class-analysis-as-a-fmm" class="slide level2">
<h2>Latent Class Analysis as a FMM</h2>
<p>A latent class model for the response vector of <span class="math inline">\(I\)</span> variables (<span class="math inline">\(i =1,\ldots,I\)</span>) with <span class="math inline">\(C\)</span> classes (<span class="math inline">\(c=1,\ldots,C\)</span>):</p>
<p><span class="math display">\[f(\boldsymbol{Y}_i) = \displaystyle {\sum_{c=1}^{C}\eta_c} \prod_{i=1}^{I} \pi_{ic}^{Y_{pi}}\left(1-\pi_{ic}\right)^{1-Y_{pi}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\eta_c\)</span> is the probability that any individual is a member of class <span class="math inline">\(c\)</span> (must sum to one)</li>
<li><span class="math inline">\(Y_{pi}\)</span> is the observed response of person <span class="math inline">\(p\)</span> to item <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\pi_{ic}\)</span> is the probability of a positive response to item <span class="math inline">\(i\)</span> from an individual from class <span class="math inline">\(c\)</span></li>
</ul>
</section>
<section id="lca-local-independence" class="slide level2">
<h2>LCA Local Independence</h2>
<ul>
<li>As shown in the LCA distributional form, LCA assumes all Bernoulli variables are independent given a class
<ul>
<li>This assumption is called Local Independence</li>
<li>It is also present in many other latent variable modeling techniques: Item response theory Factor analysis (with uncorrelated errors)</li>
</ul></li>
<li>What is implied is that any association between observed variables is accounted for only by the presence of the latent class
<ul>
<li>Essentially, this is saying that the latent class is the reason that variables are correlated</li>
</ul></li>
</ul>
</section>
<section id="estimation-process" class="slide level2">
<h2>Estimation Process</h2>
<ul>
<li>Successfully applying an LCA model to data involves the resolution to two key questions:</li>
</ul>
<ol type="1">
<li>How many classes are present?</li>
<li>What does each class represent?</li>
</ol>
<ul>
<li>The answer to the first question comes from fitting LCA models with differing numbers of classes, then choosing the model with the best fit (to be defined later)</li>
<li>The answer to the second question comes from inspecting the LCA model parameters of the solution that was deemed to have fit best</li>
</ul>
</section>
<section id="lca-estimation-software" class="slide level2">
<h2>LCA Estimation Software</h2>
<ul>
<li>There are several programs that exist that can estimate LCA models
<ul>
<li>The package to be used today will be Mplus (with the Mixture add-on)</li>
</ul></li>
<li>Other packages also exist:
<ul>
<li>Latent Gold</li>
<li>A user-developed procedure in SAS (proc lca)</li>
</ul></li>
<li>Note: Due to the likelihood function not being unimodal (a single maximum), mixture models (and LCA models) are difficult to estimate using Bayesian methods</li>
</ul>
</section></section>
<section>
<section id="lca-example-1" class="title-slide slide level1 center">
<h1>LCA Example #1</h1>

</section>
<section id="lca-example-1-1" class="slide level2">
<h2>LCA Example #1</h2>
<ul>
<li>To illustrate the process of LCA, we will use the example presented in Bartholomew and Knott (p.&nbsp;142)</li>
<li>The data are from a four-item test analyzed with an LCA by Macready and Dayton (1977)</li>
<li>The test data used by Macready and Dayton were items from a math test</li>
<li>Ultimately, Macready and Dayton wanted to see if examinees could be placed into two groups:
<ul>
<li>Those who had mastered the material</li>
<li>Those who had not mastered the material</li>
</ul></li>
</ul>

<img data-src="bk1a.png" class="r-stretch quarto-figure-center"><p class="caption">LCA Example #1</p></section>
<section id="lca-example-1-2" class="slide level2">
<h2>LCA Example #1</h2>
<ul>
<li>Several considerations will keep us from assessing the number of classes in Macready and Dayton’s data:
<ul>
<li>We only have four items</li>
<li>Macready and Dayton hypothesized two distinct classes: masters and non-masters</li>
</ul></li>
<li>For these reasons, we will only fit the two-class model and interpret the LCA model parameter estimates</li>
</ul>
</section>
<section id="mplus-input" class="slide level2">
<h2>Mplus Input</h2>
<pre><code>    TITLE:      LCA of Macready and Dayton's data (1977).
                Two classes.
    DATA:       FILE IS mddata.dat;
    VARIABLE:   NAMES ARE u1-u4;
                CLASSES = c(2);
                CATEGORICAL = u1-u4;
    ANALYSIS:   TYPE = MIXTURE;
                STARTS = 100 100;
    OUTPUT:     TECH1 TECH10;
    PLOT:       TYPE=PLOT3;
                SERIES IS u1(1) u2(2) u3(3) u4(4);
    SAVEDATA:   FORMAT IS f10.5;
                FILE IS examinee_ests.dat;
                SAVE = CPROBABILITIES;</code></pre>
</section>
<section id="lca-parameter-information-types" class="slide level2">
<h2>LCA Parameter Information Types</h2>
<ul>
<li>Recall, we have three pieces of information we can gain from an LCA:
<ul>
<li>Sample information - proportion of people in each class (<span class="math inline">\(\eta_c\)</span>)</li>
<li>Item information - probability of correct response for each item from examinees from each class (<span class="math inline">\(\pi_{ic}\)</span>)</li>
<li>Examinee information - posterior probability of class membership for each examinee in each class (<span class="math inline">\(\alpha_{pc}\)</span>)</li>
</ul></li>
</ul>
</section>
<section id="estimates-of-eta_c-from-mplus" class="slide level2">
<h2>Estimates of <span class="math inline">\(\eta_c\)</span> From Mplus:</h2>
<pre><code>    FINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES
    BASED ON THE ESTIMATED MODEL

        Latent
       Classes

           1         83.29149          0.58656
           2         58.70851          0.41344</code></pre>
<p><span class="math inline">\(\eta_c\)</span> are proportions in far right column</p>
</section>
<section id="estimates-of-pi_ic-from-mplus" class="slide level2">
<h2>Estimates of <span class="math inline">\(\pi_{ic}\)</span> From Mplus:</h2>
<pre><code>    RESULTS IN PROBABILITY SCALE
    Latent Class 1
     U1 Category 2         0.753    0.060
     U2 Category 2         0.780    0.069
     U3 Category 2         0.432    0.058
     U4 Category 2         0.708    0.063

    Latent Class 2
     U1 Category 2         0.209    0.066
     U2 Category 2         0.068    0.056
     U3 Category 2         0.018    0.037
     U4 Category 2         0.052    0.057</code></pre>
<p><span class="math inline">\(\pi_{ic}\)</span> are proportions in left column, followed by asymptotic standard errors</p>
</section>
<section id="interpreting-classes" class="slide level2">
<h2>Interpreting Classes</h2>
<ul>
<li>After the analysis is finished, we need to examine the item probabilities to gain information about the characteristics of the classes</li>
<li>An easy way to do this is to plot the item probabilities for each class</li>
</ul>

<img data-src="cp1.png" class="r-stretch"></section>
<section id="interpreting-classes-1" class="slide level2">
<h2>Interpreting Classes</h2>
<ul>
<li>Here, we would say that Class 1 represents students who have mastered the material on the test</li>
<li>We would say that Class 2 represents students who have not mastered the material on the test</li>
</ul>
</section></section>
<section>
<section id="assessing-model-fit" class="title-slide slide level1 center">
<h1>Assessing Model Fit</h1>

</section>
<section id="assessing-model-fit-1" class="slide level2">
<h2>Assessing Model Fit</h2>
<ul>
<li>As with other statistical techniques, there is no one best way to assess the fit of an LCA model</li>
<li>Techniques typically used can be put into several general categories:
<ul>
<li>Model based hypothesis tests (absolute fit)</li>
<li>Information criteria Measures based on distributional characteristics (for relative fit among a set of models)</li>
<li>Entropy (depends on absolute fit)</li>
</ul></li>
</ul>
</section>
<section id="model-based-measures" class="slide level2">
<h2>Model Based Measures</h2>
<ul>
<li>Recall the standard latent class model: A latent class model for the response vector of <span class="math inline">\(I\)</span> variables (<span class="math inline">\(i =1,\ldots,I\)</span>) with C classes (<span class="math inline">\(c=1,\ldots,C\)</span>):</li>
</ul>
<p><span class="math display">\[f(\boldsymbol{Y}_p) = \displaystyle {\sum_{c=1}^{C}\eta_c} \prod_{i=1}^{I} \pi_{ic}^{Y_{pi}} \left(1-\pi_{pi}\right)^{1-Y_{pi}}\]</span></p>
<ul>
<li>Model based measures of fit revolve around the model function listed above</li>
<li>With just the function above, we can compute the probability of <em>any</em> given response pattern</li>
<li>Mplus gives this information using the TECH10 output option</li>
</ul>
</section>
<section id="model-chi-squared-test" class="slide level2">
<h2>Model Chi-squared Test</h2>
<ul>
<li>The <span class="math inline">\(\chi^2\)</span> test compares the sets of response patterns that were observed with the set of response patterns expected under the model</li>
<li>To form the <span class="math inline">\(\chi^2\)</span> test, one must first compute the probability of each response pattern using the latent class model equation displayed on the last slide</li>
<li>The hypothesis tested is that the observed frequency is equal to the expected frequency</li>
<li>If the test has a low p-value, the model is said to not fit</li>
<li>To demonstrate the model <span class="math inline">\(\chi^2\)</span> test, let’s consider the results of the latent class model fit to the data from our running example (from Macready and Dayton, 1977)</li>
</ul>
</section>
<section id="chi-squared-test-example" class="slide level2">
<h2>Chi-squared Test Example</h2>
<pre><code>    Class Probabilities:
    Class   Probability
    1       0.587
    2       0.413

    Item Parameters
    class:         1
    item    prob    SE(prob)
    1       0.753   0.051
    2       0.780   0.051
    3       0.432   0.056
    4       0.708   0.054

    class:         2
    item    prob    SE(prob)
    1       0.209   0.060
    2       0.068   0.048
    3       0.018   0.029
    4       0.052   0.044</code></pre>
</section>
<section id="chi-squared-test-example-1" class="slide level2">
<h2>Chi-squared Test Example</h2>
<ul>
<li>To begin, compute the probability of observing the pattern <span class="math inline">\([1 1 1 1]\)</span>…</li>
<li>Then, to find the expected frequency, multiply that probability by the number of observations in the sample</li>
<li>Repeat that process for all cells…</li>
<li>Then, compute <span class="math inline">\(\chi^2_p = \displaystyle \sum_r \frac{(O_r -E_r)^2}{E_r}\)</span>, where <span class="math inline">\(r\)</span> represents each response pattern</li>
<li>The degrees of freedom are equal to the number of response patterns minus model parameters minus one</li>
<li>Then find the p-value, and decide if the model fits</li>
</ul>
</section>
<section id="chi-squared-from-mplus" class="slide level2">
<h2>Chi-squared from Mplus</h2>
<pre><code>         RESPONSE PATTERN FREQUENCIES AND CHI-SQUARE CONTRIBUTIONS

        Response          Frequency      Standard  Chi-square Contribution
         Pattern    Observed  Estimated  Residual  Pearson   Loglikelihood  Deleted
             1        41.00      41.04      0.01      0.00        -0.08
             2        13.00      12.91      0.03      0.00         0.18
             3         6.00       5.62      0.16      0.03         0.79
             4         7.00       8.92      0.66      0.41        -3.39
             5         1.00       1.30      0.27      0.07        -0.53
             6         3.00       1.93      0.77      0.59         2.63
             7         2.00       2.08      0.05      0.00        -0.15
             8         7.00       6.19      0.33      0.10         1.71
             9         4.00       4.04      0.02      0.00        -0.07
            10         6.00       6.13      0.05      0.00        -0.26
            11         5.00       6.61      0.64      0.39        -2.79
            12        23.00      19.74      0.79      0.54         7.04
            13         4.00       1.42      2.18      4.70         8.29
            14         1.00       4.22      1.59      2.46        -2.88
            15         4.00       4.90      0.41      0.16        -1.62
            16        15.00      14.95      0.01      0.00         0.09</code></pre>
</section>
<section id="likelihood-ratio-chi-squared" class="slide level2">
<h2>Likelihood Ratio Chi-squared</h2>
<ul>
<li><p>The likelihood ratio Chi-square is a variant of the Pearson Chi-squared test, but still uses the observed and expected frequencies for each cell</p></li>
<li><p>The formula for this test is: <span class="math display">\[G = 2 \sum_r O_r \ln\left(\frac{O_r}{E_r}\right)\]</span></p></li>
<li><p>The degrees of freedom are still the same as the Pearson Chi-squared test, however</p></li>
</ul>
</section>
<section id="tests-from-mplus" class="slide level2">
<h2>Tests from Mplus</h2>
<pre><code>    Chi-Square Test of Model Fit for the Binary
    and Ordered Categorical (Ordinal) Outcomes

              Pearson Chi-Square

              Value                              9.459
              Degrees of Freedom                     6
              P-Value                           0.1494

              Likelihood Ratio Chi-Square

              Value                              8.966
              Degrees of Freedom                     6
              P-Value                           0.1755</code></pre>
</section>
<section id="chi-squared-problems" class="slide level2">
<h2>Chi-squared Problems</h2>
<ul>
<li>The Chi-square test is reasonable for situations where the sample size is large, and the number of variables is small
<ul>
<li>If there are too many cells where the observed frequency is small (or zero), the test is not valid</li>
</ul></li>
<li>Note that the total number of response patterns in an LCA is <span class="math inline">\(2^I\)</span>, where <span class="math inline">\(I\)</span> is the total number of variables</li>
<li>For our example, we had four variables, so there were 16 possible response patterns</li>
<li>If we had 20 variables, there would be a total of 1,048,576 response patterns
<ul>
<li>Think about the number of observations you would have to have if you were to observe at least <em>one</em> person with each response pattern</li>
<li>Now think about if the items were highly associated (you would need even more people)</li>
</ul></li>
</ul>
</section>
<section id="model-comparison" class="slide level2">
<h2>Model Comparison</h2>
<ul>
<li>So, if model-based Chi-squared tests are valid only for a limited set of analyses, what else can be done?</li>
<li>One thing is to look at comparative measures of model fit</li>
<li>Such measures will allow the user to compare the fit of one solution (say two classes) to the fit of another (say three classes)</li>
<li>Note that such measures are only valid as a means of relative model fit - what do these measures become if the model fits perfectly?</li>
</ul>
</section>
<section id="log-likelihood" class="slide level2">
<h2>Log Likelihood</h2>
<ul>
<li>Prior to discussing anything, let’s look at the log-likelihood function, taken across all the observations in our data set</li>
<li>The log likelihood serves as the basis for the AIC and BIC, and is what is maximized by the estimation algorithm</li>
<li>The likelihood function is the model formulation across the joint distribution of the data (all observations):</li>
</ul>
<p><span class="math display">\[L(\boldsymbol{Y}_p) =\prod_{p=1}^N  \left[\displaystyle {\sum_{c=1}^{C}\eta_c} \prod_{i=1}^{I} \pi_{ic}^{Y_{pi}} \left(1-\pi_{pi}\right)^{1-Y_{pi}} \right]\]</span></p>
</section>
<section id="log-likelihood-1" class="slide level2">
<h2>Log Likelihood</h2>
<ul>
<li>The log likelihood function is the log of the model formulation across the joint distribution of the data (all observations):</li>
</ul>
<p><span class="math display">\[Log L(\boldsymbol{Y}_{pi}) =\log \left(\prod_{p=1}^N  \left[\displaystyle {\sum_{c=1}^{C}\eta_c} \prod_{i=1}^{I} \pi_{ic}^{Y_{pi}} \left(1-\pi_{pi}\right)^{1-Y_{pi}} \right]\right)\]</span></p>
<p><span class="math display">\[Log L(\boldsymbol{Y}_{pi}) = \sum_{p=1}^N \log \left( \displaystyle {\sum_{c=1}^{C}\eta_c} \prod_{i=1}^{I} \pi_{ic}^{Y_{pi}} \left(1-\pi_{pi}\right)^{1-Y_{pi}}\right)\]</span></p>
<ul>
<li><p>Here, the log function taken is typically base <span class="math inline">\(e\)</span> - the natural log</p></li>
<li><p>The log likelihood is a function of the observed responses for each person and the model parameters</p></li>
</ul>
</section>
<section id="information-criteria" class="slide level2">
<h2>Information Criteria</h2>
<ul>
<li>The Akaike Information Criterion (AIC) is a measure of the goodness of fit of a model that considers the number of model parameters (q)</li>
</ul>
<p><span class="math display">\[AIC = 2q - 2 \log L\]</span></p>
<ul>
<li>Schwarz’s Information Criterion (also called the Bayesian Information Criterion or the Schwarz-Bayesian Information Criterion) is a measure of the goodness of fit of a model that considers the number of parameters (q) and the number of observations (N):</li>
</ul>
<p><span class="math display">\[BIC = q \log (N) - 2 \log L\]</span></p>
</section>
<section id="fit-from-mplus" class="slide level2">
<h2>Fit from Mplus</h2>
<pre><code>    TESTS OF MODEL FIT

    Loglikelihood

              H0 Value                        -331.764

    Information Criteria

              Number of Free Parameters              9
              Akaike (AIC)                     681.527
              Bayesian (BIC)                   708.130
              Sample-Size Adjusted BIC         679.653
                (n* = (n + 2) / 24)
              Entropy                            0.754</code></pre>
</section>
<section id="information-criteria-1" class="slide level2">
<h2>Information Criteria</h2>
<ul>
<li>When considering which model “fits” the data best, the model with the lowest AIC or BIC should be considered</li>
<li>Although AIC and BIC are based on good statistical theory, neither is a gold standard for assessing which model should be chosen</li>
<li>Furthermore, neither will tell you, overall, if your model estimates bear any decent resemblance to your data</li>
<li>You could be choosing between two (equally) poor models - other measures are needed</li>
</ul>
</section>
<section id="distributional-measures-of-model-fit" class="slide level2">
<h2>Distributional Measures of Model Fit</h2>
<ul>
<li>The model-based Chi-squared provided a measure of model fit, while narrow in the times it could be applied, that tried to map what the model said the data looked like to what the data actually looked like</li>
<li>The same concept lies behind the ideas of distributional measures of model fit - use the parameters of the model to “predict” what the data should look like</li>
<li>In this case, measures that are easy to attain are measures that look at:
<ul>
<li>Each variable marginally - the mean (or proportion)</li>
<li>The bivariate distribution of each pair of variables - contingency tables (for categorical variables), correlation</li>
<li>matrices, or covariance matrices</li>
</ul></li>
</ul>
</section>
<section id="marginal-measures" class="slide level2">
<h2>Marginal Measures</h2>
<ul>
<li>For each item, the model-predicted mean of the item (proportion of people responding with a value of one) is given by:</li>
</ul>
<p><span class="math display">\[\hat{\bar{Y}}_i = \hat{E}(Y_i) = \sum_{c=1}^J \hat{\eta}_c \times \hat{\pi}_{ic}\]</span></p>
<ul>
<li><p>Across all items, you can then form an aggregate measure of model fit by comparing the observed mean of the item to that found under the model, such as the root mean squared error (note: this is not RMSEA from CFA/IFA): <span class="math display">\[RMSE = \sqrt{\frac{\sum_{i=1}^I(\hat{\bar{Y}}_i - \bar{Y}_i)^2}{I}}\]</span></p></li>
<li><p>Often, there is not much difference between observed and predicted mean (depending on the model, the fit will always be perfect)</p></li>
</ul>
</section>
<section id="marginal-measures-from-mplus-from-mplus-using-tech10" class="slide level2">
<h2>Marginal Measures from Mplus From Mplus (using TECH10):</h2>
<pre><code>         UNIVARIATE MODEL FIT INFORMATION

                                   Estimated Probabilities
         Variable              H1           H0    Standard Residual
         U1
           Category 1        0.472        0.472         0.000
           Category 2        0.528        0.528         0.000
         U2
           Category 1        0.514        0.514         0.000
           Category 2        0.486        0.486         0.000
         U3
           Category 1        0.739        0.739         0.000
           Category 2        0.261        0.261         0.000
         U4
           Category 1        0.563        0.563         0.000
           Category 2        0.437        0.437         0.000</code></pre>
</section>
<section id="bivariate-measures" class="slide level2">
<h2>Bivariate Measures</h2>
<ul>
<li>For a pair of items (say <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the model-predicted probability of both being one is given in the same way:</li>
</ul>
<p><span class="math display">\[\hat{P}(Y_a = 1, Y_b=1) = \sum_{c=1}^C \hat{\eta}_c \times \hat{\pi}_{ac} \times \hat{\pi}_{bc}\]</span></p>
<ul>
<li>Given the marginal means, you can now form a 2 x 2 table of the probability of finding a given pair of responses to variable <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></li>
</ul>
</section>
<section id="bivariate-measures-1" class="slide level2">
<h2>Bivariate Measures</h2>
<ul>
<li>Given the model-predicted contingency table (on the last slide) for every pair of items, you can then form a measure of association for the items</li>
<li>There are multiple ways to summarize association in a contingency table</li>
<li>Depending on your preference, you could use:
<ul>
<li>Pearson correlation</li>
<li>Tetrachoric correlation</li>
<li>Cohen’s kappa</li>
</ul></li>
<li>After that, you could then summarize the discrepancy between what your model predicts and what you have observed in the data
<ul>
<li>Such as the RMSE, MAD, or BIAS</li>
</ul></li>
</ul>
</section>
<section id="bivariate-measures-from-mplus-from-mplus-using-tech10" class="slide level2">
<h2>Bivariate Measures from Mplus From Mplus (using TECH10):</h2>
<pre><code>         BIVARIATE MODEL FIT INFORMATION

                                                  Estimated Probabilities
         Variable       Variable              H1           H0     Standard Residual
         U1             U2
           Category 1     Category 1        0.352        0.337         0.391
           Category 1     Category 2        0.120        0.135        -0.540
           Category 2     Category 1        0.162        0.177        -0.483
           Category 2     Category 2        0.366        0.351         0.387</code></pre>
</section>
<section id="entropy" class="slide level2">
<h2>Entropy</h2>
<ul>
<li>The entropy of a model is defined to be a measure of classification uncertainty</li>
<li>To define the entropy of a model, we must first look at the posterior probability of class membership, let’s call this <span class="math inline">\(\hat{\alpha}_{ic}\)</span></li>
<li>Here, <span class="math inline">\(\hat{\alpha}_{ic}\)</span> is the estimated probability that observation <span class="math inline">\(i\)</span> is a member of class <span class="math inline">\(c\)</span></li>
<li>The entropy of a model is defined as:</li>
</ul>
<p><span class="math display">\[EN(\boldsymbol{\alpha}) = - \sum_{i=1}^N \sum_{c=1}^C \hat{\alpha}_{ic} \log \hat{\alpha}_{ic}\]</span></p>
</section>
<section id="relative-entropy" class="slide level2">
<h2>Relative Entropy</h2>
<ul>
<li><p>The entropy equation on the last slide is bounded from <span class="math inline">\([0,\infty)\)</span>, with higher values indicated a larger amount of uncertainty in classification</p></li>
<li><p>Mplus reports the <em>relative</em> entropy of a model, which is a rescaled version of entropy:</p>
<p><span class="math display">\[E = 1 - \frac{EN(\boldsymbol{\alpha})}{N \log C}\]</span></p></li>
<li><p>The relative entropy is defined on <span class="math inline">\([0,1]\)</span>, with values near one indicating high certainty in classification and values near zero indicating low certainty</p></li>
</ul>
</section>
<section id="fit-from-mplus-1" class="slide level2">
<h2>Fit from Mplus</h2>
<pre><code>    TESTS OF MODEL FIT

    Loglikelihood

              H0 Value                        -331.764

    Information Criteria

              Number of Free Parameters              9
              Akaike (AIC)                     681.527
              Bayesian (BIC)                   708.130
              Sample-Size Adjusted BIC         679.653
                (n* = (n + 2) / 24)
              Entropy                            0.754</code></pre>
</section></section>
<section>
<section id="from-lca-to-more-useful-fmms" class="title-slide slide level1 center">
<h1>From LCA to More Useful FMMs</h1>

</section>
<section id="general-notational-framework" class="slide level2">
<h2>General Notational Framework</h2>
<p>The finite mixture model is a general framework for modeling data that can be expressed as:</p>
<p><span class="math display">\[f(\boldsymbol{Y}) = \sum_{c=1}^C \eta_c f(\boldsymbol{Y}|c)\]</span></p>
<ul>
<li>Where <span class="math inline">\(\boldsymbol{Y}\)</span> is a vector of observed variables</li>
<li><span class="math inline">\(f(\boldsymbol{Y})\)</span> is the joint distribution of the observed variables</li>
<li><span class="math inline">\(f(\boldsymbol{Y}|c)\)</span> is the conditional distribution of the observed variables given class <span class="math inline">\(c\)</span>
<ul>
<li>The distributional form of <span class="math inline">\(f(\boldsymbol{Y}|c)\)</span> is dependent on the type of observed variables</li>
</ul></li>
<li><span class="math inline">\(\eta_c\)</span> is the probability of class membership for class <span class="math inline">\(c\)</span></li>
<li><span class="math inline">\(C\)</span> is the number of classes</li>
</ul>
</section>
<section id="mixture-irt-models" class="slide level2">
<h2>Mixture IRT Models</h2>
<ul>
<li>Perhaps most relevant to this class is are the family of models that go by the term “Mixture Item Response Models”
<ul>
<li>Most popular: Mixture Rasch Models</li>
</ul></li>
<li>The general form of a mixture IRT model for binary items is:</li>
</ul>
<p><span class="math display">\[f(\boldsymbol{Y}_p \mid \theta_p) = \sum_{c=1}^C \eta_c f(\boldsymbol{Y}_p|c, \theta_p)\]</span></p>
<p>Where:</p>
<p><span class="math display">\[f(\boldsymbol{Y}|c) = \prod_{i=1}^I \pi_{ic}^{Y_{pi}} \left( 1-\pi_{ic} \right)^{1-Y_{pi}}\]</span></p>
<p>And:</p>
<p><span class="math display">\[\pi_{ic} = P \left( Y_{pi} = 1 \mid \theta_p \right) = \frac{\exp \left( a_{ic} \left( \theta_p - b_{ic}\right) \right)}{1+\exp \left( a_{ic} \left( \theta_p - b_{ic}\right) \right)}\]</span></p>
</section>
<section id="mixture-irt-model-parameters" class="slide level2">
<h2>Mixture IRT Model Parameters</h2>
<ul>
<li>The parameters of a mixture IRT model are:
<ul>
<li><span class="math inline">\(\eta_c\)</span> - the probability of class membership for class <span class="math inline">\(c\)</span></li>
<li><span class="math inline">\(a_{ic}\)</span> - the discrimination parameter for item <span class="math inline">\(i\)</span> in class <span class="math inline">\(c\)</span></li>
<li><span class="math inline">\(b_{ic}\)</span> - the difficulty parameter for item <span class="math inline">\(i\)</span> in class <span class="math inline">\(c\)</span></li>
</ul></li>
</ul>
</section>
<section id="mixture-irt-model-motivation" class="slide level2">
<h2>Mixture IRT Model Motivation</h2>
<ul>
<li>Here, several researchers had the belief that mixture IRT models would reveal differing strategies used by examinees on items
<ul>
<li>But, that’s assuming that the classes are meaningful</li>
</ul></li>
<li>In Templin and Alexeev (2011), we showed that classes were often formed when a single-class IRT model misfit the data
<ul>
<li>Classes can be spurious</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="latent-class-analysis-wrap-up" class="title-slide slide level1 center">
<h1>Latent Class Analysis: Wrap Up</h1>

</section>
<section id="lca-limitations" class="slide level2">
<h2>LCA Limitations</h2>
<ul>
<li>LCA has limitations which can make its general application difficult:
<ul>
<li>Classes not known prior to analysis</li>
<li>Class characteristics not know until after analysis</li>
</ul></li>
<li>Both of these problems are related to LCA being an exploratory procedure for understanding data</li>
<li>Diagnostic Classification Models can be thought of as one type of a confirmatory LCA
<ul>
<li>By placing constraints on the class item probabilities and specifying what our classes mean prior to analysis</li>
</ul></li>
</ul>
</section>
<section id="lca-summary" class="slide level2">
<h2>LCA Summary</h2>
<ul>
<li>Latent class analysis is a model-based technique for finding clusters in binary (categorical) data</li>
<li>Each of the variables is assumed to:
<ul>
<li>Have a Bernoulli distribution</li>
<li>Be independent given class</li>
</ul></li>
<li>Additional reading (book): Lazarsfeld and Henry (1968). Latent structure analysis</li>
</ul>
<div class="footer footer-default">
<p>Multidimensional Measurement Models (Fall 2023): Lecture 10</p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="10_Mixture_Models_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="10_Mixture_Models_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="10_Mixture_Models_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>